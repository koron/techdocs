{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76ab823-5cad-495e-8fd4-fa8f939a507f",
   "metadata": {},
   "source": [
    "# 本ディレクトリの一連の流れを試せるノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573ae0a-4837-43d0-9f46-55b5bdb781f8",
   "metadata": {},
   "source": [
    "## 諸々の準備\n",
    "\n",
    "まずはインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0553674-7432-4341-a2c3-81fe0c0a791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle\n",
    "import torch\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Operator\n",
    "\n",
    "from merlin.dag import ColumnSelector\n",
    "from merlin.schema import Schema, Tags\n",
    "from merlin.schema import Schema\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# numba からの警告を抑制する\n",
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0\n",
    "\n",
    "import transformers4rec.torch as tr\n",
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt\n",
    "from transformers4rec.torch.utils.examples_utils import wipe_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224e07e-f29f-4898-b228-10bdb1174c12",
   "metadata": {},
   "source": [
    "各種定数を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ca665a-3147-4087-ab40-b54397261afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data/\")\n",
    "CSV_PATH = os.path.join(INPUT_DATA_DIR, \"2019-Oct.csv\")\n",
    "PARQUET_PATH = os.path.join(INPUT_DATA_DIR, \"Oct-2019.parquet\")\n",
    "PROCESSED_PATH = os.path.join(INPUT_DATA_DIR, \"processed_nvt\")\n",
    "WORKFLOW_PATH = os.path.join(INPUT_DATA_DIR, 'workflow_etl')\n",
    "SESSIONS_PATH = os.path.join(INPUT_DATA_DIR, \"sessions_by_day\")\n",
    "MODEL_PATH = os.path.join(INPUT_DATA_DIR, \"trained_model\")\n",
    "\n",
    "SESSIONS_MAX_LENGTH = 20\n",
    "MINIMUM_SESSION_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67394f2-7c45-4b8e-92aa-a8d67fba05cb",
   "metadata": {},
   "source": [
    "## 学習データの準備\n",
    "\n",
    "実験のためのデータをダウンロードする。\n",
    "使うデータは <https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store> の `2019-Oct.csv` で、\n",
    "サイズは5.3GiBになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9149dce-a9da-46e3-aa02-eb3f040426f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials set.\n",
      "Kaggle credentials successfully validated.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CSV_PATH):\n",
    "    # データをCSV_PATHへダウンロードする\n",
    "    import kagglehub\n",
    "    import shutil\n",
    "\n",
    "    # 環境変数 KAGGLE_USERNAME と KAGGLE_KEY でログイン。\n",
    "    # 環境変数が設定していない場合は下記の YOUR XXX HERE の部分を該当する値へ書き換えても良い\n",
    "    KAGGLE_USERNAME = os.environ.get('KAGGLE_USERNAME', 'YOUR USERNAME HERE')\n",
    "    KAGGLE_KEY = os.environ.get('KAGGLE_KEY', 'YOUR API KEY HERE')\n",
    "    kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
    "    kagglehub.whoami()\n",
    "\n",
    "    # 2019-Oct.csv をダウンロードし、ワーキングディレクトリにコピー\n",
    "    path = kagglehub.dataset_download('mkechinov/ecommerce-behavior-data-from-multi-category-store', path='2019-Oct.csv')\n",
    "    shutil.copy2(path, CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944950f-04ce-4379-9a9b-efabf97739fd",
   "metadata": {},
   "source": [
    "CSVのままでは扱いにくいので Parquet 形式に変換する。\n",
    "初めて変換する場合は特にDocker環境では5分ほどかかる。\n",
    "変換結果はファイルに保存するので、そのファイル `Oct-2019.parquet` がある場合は自動的にこのステップをスキップする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3757a1-f68e-4242-8f00-62bd694555cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with in-session repeated interactions: 42448762\n",
      "Count after removed in-session repeated interactions: 30733301\n",
      "CPU times: user 59.1 s, sys: 16.2 s, total: 1min 15s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(PARQUET_PATH):\n",
    "    # CSVをロードする。Docker上でやると3～5分くらいかかる\n",
    "    raw_df = cudf.read_csv(CSV_PATH)\n",
    "    \n",
    "    # タイムスタンプの形式を秒へ変換\n",
    "    raw_df['event_time_dt'] = raw_df['event_time'].astype('datetime64[s]')\n",
    "    raw_df['event_time_ts'] = raw_df['event_time_dt'].astype('int')\n",
    "    \n",
    "    # `user_session` カラムが null の行を削除\n",
    "    raw_df = raw_df[raw_df['user_session'].isnull()==False]\n",
    "    \n",
    "    # `event_time` カラムは利用しない\n",
    "    raw_df = raw_df.drop(['event_time'], axis=1)\n",
    "    \n",
    "    # Workflowを用いて `user_session` カラムでグルーピングしデータフレームへ変換する\n",
    "    cols = list(raw_df.columns)\n",
    "    cols.remove('user_session')\n",
    "    df_event = nvt.Dataset(raw_df) \n",
    "    cat_feats = ['user_session'] >> nvt.ops.Categorify()\n",
    "    workflow = nvt.Workflow(cols + cat_feats)\n",
    "    workflow.fit(df_event)\n",
    "    df = workflow.transform(df_event).to_ddf().compute()\n",
    "    \n",
    "    # データ読み込みに利用していたメモリを解放する\n",
    "    raw_df = None\n",
    "    del(raw_df)\n",
    "    gc.collect()\n",
    "    \n",
    "    # 連続した (user, item) のインタラクションを削除\n",
    "    df = df.sort_values(['user_session', 'event_time_ts']).reset_index(drop=True)\n",
    "    print(\"Count with in-session repeated interactions: {}\".format(len(df)))\n",
    "    # Sorts the dataframe by session and timestamp, to remove consecutive repetitions\n",
    "    df['product_id_past'] = df['product_id'].shift(1).fillna(0)\n",
    "    df['session_id_past'] = df['user_session'].shift(1).fillna(0)\n",
    "    #Keeping only no consecutive repeated in session interactions\n",
    "    df = df[~((df['user_session'] == df['session_id_past']) & \\\n",
    "                 (df['product_id'] == df['product_id_past']))]\n",
    "    print(\"Count after removed in-session repeated interactions: {}\".format(len(df)))\n",
    "    del(df['product_id_past'])\n",
    "    del(df['session_id_past'])\n",
    "    gc.collect()\n",
    "    \n",
    "    # 特定の item が最初に表れた時刻を記録するカラムを追加\n",
    "    item_first_interaction_df = df.groupby('product_id').agg({'event_time_ts': 'min'}) \\\n",
    "                .reset_index().rename(columns={'event_time_ts': 'prod_first_event_time_ts'})\n",
    "    gc.collect()\n",
    "    df = df.merge(item_first_interaction_df, on=['product_id'], how='left').reset_index(drop=True)\n",
    "    item_first_interaction_df=None\n",
    "    del(item_first_interaction_df)\n",
    "    gc.collect()\n",
    "\n",
    "    # 最初の1週間分のデータだけを使う\n",
    "    df = df[df['event_time_dt'] < np.datetime64('2019-10-08')].reset_index(drop=True)\n",
    "    # それが済めば `event_time_dt` カラムは不要なので削除する\n",
    "    df = df.drop(['event_time_dt'], axis=1)\n",
    "\n",
    "    # Parquet形式にして書き出す\n",
    "    df.to_parquet(PARQUET_PATH)\n",
    "\n",
    "    df = None\n",
    "    del(df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0f903-35f3-4d0b-bfe1-f1f870f5647f",
   "metadata": {},
   "source": [
    "## Workflowを使って ETL する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13631ae7-fa02-4612-b962-c0c4b7d5514a",
   "metadata": {},
   "source": [
    "以下はWorkflowを構築する手続き。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80bd35c-49dd-4d3d-839e-bf05325fafad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 24.7 ms, total: 139 ms\n",
      "Wall time: 625 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# カテゴリ化するカラムを指定する\n",
    "item_id = ['product_id'] >> nvt.ops.TagAsItemID()\n",
    "cat_feats = item_id + ['category_code', 'brand', 'user_id', 'category_id', 'event_type'] >> nvt.ops.Categorify()\n",
    "\n",
    "\n",
    "# 時刻に関するカラムを変換する\n",
    "\n",
    "session_ts = ['event_time_ts']\n",
    "\n",
    "session_time = (\n",
    "    session_ts >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "\n",
    "sessiontime_weekday = (\n",
    "    session_time >> \n",
    "    nvt.ops.LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    nvt.ops.Rename(name ='et_dayofweek')\n",
    ")\n",
    "\n",
    "\n",
    "def get_cycled_feature_value_sin(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_sin = np.sin(2*np.pi*value_scaled)\n",
    "    return value_sin\n",
    "\n",
    "def get_cycled_feature_value_cos(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_cos = np.cos(2*np.pi*value_scaled)\n",
    "    return value_cos\n",
    "\n",
    "weekday_sin = (sessiontime_weekday >> \n",
    "               (lambda col: get_cycled_feature_value_sin(col+1, 7)) >> \n",
    "               nvt.ops.Rename(name = 'et_dayofweek_sin') >>\n",
    "               nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    "              )\n",
    "    \n",
    "weekday_cos= (sessiontime_weekday >> \n",
    "              (lambda col: get_cycled_feature_value_cos(col+1, 7)) >> \n",
    "              nvt.ops.Rename(name = 'et_dayofweek_cos') >>\n",
    "              nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    "             )\n",
    "\n",
    "# アイテムの最新性を計算するためのカスタムオペレーター\n",
    "class ItemRecency(nvt.ops.Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "        for column in columns.names:\n",
    "            col = gdf[column]\n",
    "            item_first_timestamp = gdf['prod_first_event_time_ts']\n",
    "            delta_days = (col - item_first_timestamp) / (60*60*24)\n",
    "            gdf[column + \"_age_days\"] = delta_days * (delta_days >=0)\n",
    "        return gdf\n",
    "\n",
    "    def compute_selector(\n",
    "        self,\n",
    "        input_schema: Schema,\n",
    "        selector: ColumnSelector,\n",
    "        parents_selector: ColumnSelector,\n",
    "        dependencies_selector: ColumnSelector,\n",
    "    ) -> ColumnSelector:\n",
    "        self._validate_matching_cols(input_schema, parents_selector, \"computing input selector\")\n",
    "        return parents_selector\n",
    "\n",
    "    def column_mapping(self, col_selector):\n",
    "        column_mapping = {}\n",
    "        for col_name in col_selector.names:\n",
    "            column_mapping[col_name + \"_age_days\"] = [col_name]\n",
    "        return column_mapping\n",
    "\n",
    "    @property\n",
    "    def dependencies(self):\n",
    "        return [\"prod_first_event_time_ts\"]\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        return np.float64\n",
    "\n",
    "recency_features = ['event_time_ts'] >> ItemRecency() \n",
    "recency_features_norm = (recency_features >> \n",
    "                         nvt.ops.LogOp() >> \n",
    "                         nvt.ops.Normalize(out_dtype=np.float32) >> \n",
    "                         nvt.ops.Rename(name='product_recency_days_log_norm')\n",
    "                        )\n",
    "\n",
    "time_features = (\n",
    "    session_time +\n",
    "    sessiontime_weekday +\n",
    "    weekday_sin +\n",
    "    weekday_cos +\n",
    "    recency_features_norm\n",
    ")\n",
    "\n",
    "# ロング・テイルな価格のカラムを標準化する\n",
    "price_log = ['price'] >> nvt.ops.LogOp() >> nvt.ops.Normalize(out_dtype=np.float32) >> nvt.ops.Rename(name='price_log_norm')\n",
    "\n",
    "# 相対価格をカテゴリー(ID)の平均に変換する\n",
    "\n",
    "def relative_price_to_avg_categ(col, gdf):\n",
    "    epsilon = 1e-5\n",
    "    col = ((gdf['price'] - col) / (col + epsilon)) * (col > 0).astype(int)\n",
    "    return col\n",
    "    \n",
    "avg_category_id_pr = ['category_id'] >> nvt.ops.JoinGroupby(cont_cols =['price'], stats=[\"mean\"]) >> nvt.ops.Rename(name='avg_category_id_price')\n",
    "relative_price_to_avg_category = (\n",
    "    avg_category_id_pr >> \n",
    "    nvt.ops.LambdaOp(relative_price_to_avg_categ, dependency=['price']) >> \n",
    "    nvt.ops.Rename(name=\"relative_price_to_avg_categ_id\") >>\n",
    "    nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    ")\n",
    "\n",
    "# インタラクションをセッションでグルーピングする\n",
    "groupby_feats = ['event_time_ts', 'user_session'] + cat_feats + time_features + price_log + relative_price_to_avg_category\n",
    "\n",
    "groupby_features = groupby_feats >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"user_session\"], \n",
    "    sort_cols=[\"event_time_ts\"],\n",
    "    aggs={\n",
    "        'user_id': ['first'],\n",
    "        'product_id': [\"list\", \"count\"],\n",
    "        'category_code': [\"list\"],  \n",
    "        'brand': [\"list\"], \n",
    "        'category_id': [\"list\"], \n",
    "        'event_time_ts': [\"first\"],\n",
    "        'event_time_dt': [\"first\"],\n",
    "        'et_dayofweek_sin': [\"list\"],\n",
    "        'et_dayofweek_cos': [\"list\"],\n",
    "        'price_log_norm': [\"list\"],\n",
    "        'relative_price_to_avg_categ_id': [\"list\"],\n",
    "        'product_recency_days_log_norm': [\"list\"]\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "\n",
    "groupby_features_list = groupby_features['product_id-list',\n",
    "        'category_code-list',  \n",
    "        'brand-list', \n",
    "        'category_id-list', \n",
    "        'et_dayofweek_sin-list',\n",
    "        'et_dayofweek_cos-list',\n",
    "        'price_log_norm-list',\n",
    "        'relative_price_to_avg_categ_id-list',\n",
    "        'product_recency_days_log_norm-list']\n",
    "\n",
    "groupby_features_trim = groupby_features_list >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH, pad=True)\n",
    "\n",
    "# calculate session day index based on 'timestamp-first' column\n",
    "day_index = ((groupby_features['event_time_dt-first'])  >> \n",
    "             nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days +1) >> \n",
    "             nvt.ops.Rename(f = lambda col: \"day_index\") >>\n",
    "             nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    "            )\n",
    "\n",
    "sess_id = groupby_features['user_session'] >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    "\n",
    "selected_features = sess_id + groupby_features['product_id-count'] + groupby_features_trim + day_index\n",
    "\n",
    "# 変換済みのデータを再度、読み込む\n",
    "df = cudf.read_parquet(PARQUET_PATH)\n",
    "\n",
    "filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df[\"product_id-count\"] >= MINIMUM_SESSION_LENGTH)\n",
    "\n",
    "workflow = nvt.Workflow(filtered_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f372c7-7a15-45b5-9236-27fb4c477594",
   "metadata": {},
   "source": [
    "作ったWorkflowで ETL (Extract, Transform, Load) する。\n",
    "結果は `processed_nvt` ディレクトリに保存する。\n",
    "既に `processed_nvt` ディレクトリが存在する場合は自動的にこのステップをスキップする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab5050a-efd6-4a9c-8f15-dcdd10e81018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.37 s, sys: 430 ms, total: 7.8 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(PROCESSED_PATH):\n",
    "    dataset = nvt.Dataset(df)\n",
    "    # Learn features statistics necessary of the preprocessing workflow\n",
    "    # The following will generate schema.pbtxt file in the provided folder and export the parquet files.\n",
    "    workflow.fit_transform(dataset).to_parquet(PROCESSED_PATH)\n",
    "    dataset = None\n",
    "    del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893d1ecd-84a3-4b0a-a02d-68eb956ff38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作った Workflow はディレクトリへ保存しておく\n",
    "if not os.path.exists(WORKFLOW_PATH):\n",
    "    workflow.save(WORKFLOW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4dcc2-0ae4-4154-87e9-476c7e0117a6",
   "metadata": {},
   "source": [
    "## データを日ごとに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d757a3e-533b-4c77-a1ed-8d1702a27353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|█████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 s, sys: 527 ms, total: 4.19 s\n",
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SESSIONS_PATH = os.path.join(INPUT_DATA_DIR, \"sessions_by_day\")\n",
    "\n",
    "if not os.path.exists(SESSIONS_PATH):\n",
    "    PARTITION_COL = 'day_index'\n",
    "    \n",
    "    OUTPUT_FOLDER = os.environ.get(\"OUTPUT_FOLDER\", SESSIONS_PATH)\n",
    "    !mkdir -p $OUTPUT_FOLDER\n",
    "    \n",
    "    # read in the processed train dataset\n",
    "    sessions_gdf = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
    "    \n",
    "    from transformers4rec.utils.data_utils import save_time_based_splits\n",
    "    save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
    "                           output_dir= OUTPUT_FOLDER,\n",
    "                           partition_col=PARTITION_COL,\n",
    "                           timestamp_col='user_session', \n",
    "                          )\n",
    "\n",
    "    sessions_gdf = None\n",
    "    del(sessions_gdf)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc714197-2fb5-4ba2-bdf5-5af8ccaf20f0",
   "metadata": {},
   "source": [
    "## Transformerモデルを作り学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e561809-17b8-4d31-a073-d66e3b2121db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting inputs of NextItemPredictionTask to'64' As weight tying requires the input dimension '192' to be equal to the item-id embedding dimension '64'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1314/1314 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>9.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>8.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>8.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>8.419300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1285' max='415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [415/415 01:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 2 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 8.533370971679688\n",
      " eval_/next-item/ndcg_at_10 = 0.0573887936770916\n",
      " eval_/next-item/ndcg_at_20 = 0.06957186013460159\n",
      " eval_/next-item/recall_at_10 = 0.10602518916130066\n",
      " eval_/next-item/recall_at_20 = 0.1544378250837326\n",
      " eval_runtime = 6.2887\n",
      " eval_samples_per_second = 2111.717\n",
      " eval_steps_per_second = 65.991\n",
      "********************\n",
      "Launch training for day 2 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1245' max='1245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1245/1245 00:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>8.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>8.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>7.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>7.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>7.556000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 3 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 7.9406561851501465\n",
      " eval_/next-item/ndcg_at_10 = 0.076751708984375\n",
      " eval_/next-item/ndcg_at_20 = 0.09352560341358185\n",
      " eval_/next-item/recall_at_10 = 0.1468765288591385\n",
      " eval_/next-item/recall_at_20 = 0.2135867029428482\n",
      " eval_runtime = 6.5685\n",
      " eval_samples_per_second = 1870.76\n",
      " eval_steps_per_second = 58.461\n",
      "********************\n",
      "Launch training for day 3 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>7.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>7.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.141400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 4 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 7.500641822814941\n",
      " eval_/next-item/ndcg_at_10 = 0.09556645154953003\n",
      " eval_/next-item/ndcg_at_20 = 0.11623960733413696\n",
      " eval_/next-item/recall_at_10 = 0.17901314795017242\n",
      " eval_/next-item/recall_at_20 = 0.2610152065753937\n",
      " eval_runtime = 10.9207\n",
      " eval_samples_per_second = 1424.088\n",
      " eval_steps_per_second = 44.503\n",
      "CPU times: user 3min 25s, sys: 31.9 s, total: 3min 57s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):    \n",
    "    # 入力スキーマの構築\n",
    "    \n",
    "    # Define categorical and continuous columns to fed to training model\n",
    "    x_cat_names = ['product_id-list', 'category_id-list', 'brand-list']\n",
    "    x_cont_names = ['product_recency_days_log_norm-list', 'et_dayofweek_sin-list', 'et_dayofweek_cos-list', \n",
    "                    'price_log_norm-list', 'relative_price_to_avg_categ_id-list']\n",
    "    \n",
    "    train = Dataset(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
    "    schema = train.schema\n",
    "    schema = schema.select_by_name(x_cat_names + x_cont_names)\n",
    "    \n",
    "    # モデル情報の構築\n",
    "    \n",
    "    # Define input block\n",
    "    sequence_length, d_model = 20, 192\n",
    "    # Define input module to process tabular input-features and to prepare masked inputs\n",
    "    inputs= tr.TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=sequence_length,\n",
    "        aggregation=\"concat\",\n",
    "        d_output=d_model,\n",
    "        masking=\"mlm\",\n",
    "    )\n",
    "    \n",
    "    # Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "    transformer_config = tr.XLNetConfig.build(\n",
    "        d_model=d_model, n_head=4, n_layer=2, total_seq_length=sequence_length\n",
    "    )\n",
    "    # Define the model block including: inputs, masking, projection and transformer block.\n",
    "    body = tr.SequentialBlock(\n",
    "        inputs, tr.MLPBlock([192]), tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    "    )\n",
    "    \n",
    "    # Define the head related to next item prediction task \n",
    "    head = tr.Head(\n",
    "        body,\n",
    "        tr.NextItemPredictionTask(weight_tying=True, \n",
    "                                         metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),  \n",
    "                                                  RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n",
    "    )\n",
    "    \n",
    "    # Get the end-to-end Model class \n",
    "    model = tr.Model(head)\n",
    "    \n",
    "    #Set arguments for training\n",
    "    training_args = T4RecTrainingArguments(\n",
    "                output_dir = \"./tmp\",\n",
    "                max_sequence_length=20,\n",
    "                data_loader_engine='merlin',\n",
    "                num_train_epochs=3,\n",
    "                dataloader_drop_last=False,\n",
    "                per_device_train_batch_size = 256,\n",
    "                per_device_eval_batch_size = 32,\n",
    "                gradient_accumulation_steps = 1,\n",
    "                learning_rate=0.000666,\n",
    "                report_to = [],\n",
    "                logging_steps=200,\n",
    "            )\n",
    "    \n",
    "    # 学習を実行し、結果を保存する\n",
    "    \n",
    "    # Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        schema=schema,\n",
    "        compute_metrics=True,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", SESSIONS_PATH)\n",
    "    \n",
    "    start_time_window_index = 1\n",
    "    final_time_window_index = 4\n",
    "    for time_index in range(start_time_window_index, final_time_window_index):\n",
    "        # Set data \n",
    "        time_index_train = time_index\n",
    "        time_index_eval = time_index + 1\n",
    "        train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n",
    "        eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
    "        # Train on day related to time_index \n",
    "        print('*'*20)\n",
    "        print(\"Launch training for day %s are:\" %time_index)\n",
    "        print('*'*20 + '\\n')\n",
    "        trainer.train_dataset_or_path = train_paths\n",
    "        trainer.reset_lr_scheduler()\n",
    "        trainer.train()\n",
    "        trainer.state.global_step +=1\n",
    "        # Evaluate on the following day\n",
    "        trainer.eval_dataset_or_path = eval_paths\n",
    "        train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "        print('*'*20)\n",
    "        print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "        print('\\n' + '*'*20 + '\\n')\n",
    "        for key in sorted(train_metrics.keys()):\n",
    "            print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "        wipe_memory()\n",
    "    \n",
    "    model.save(MODEL_PATH)\n",
    "\n",
    "    model = None\n",
    "    del(model)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5047a-a6ec-4aee-8785-1abbd09cc5ab",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n",
    "人工的に作ったセッションデータから、推薦アイテム列を推論(作成)する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce69f39-7e41-4900-81e0-6c37cc957fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose dataframe\n",
    "\n",
    "cols = [\n",
    "        'product_id-list',\n",
    "        'brand-list',\n",
    "        'category_id-list',\n",
    "        'et_dayofweek_sin-list',\n",
    "        'et_dayofweek_cos-list',\n",
    "        'price_log_norm-list',\n",
    "        'relative_price_to_avg_categ_id-list',\n",
    "        'product_recency_days_log_norm-list',\n",
    "        ]\n",
    "\n",
    "emp = np.empty(0, dtype=np.int64)\n",
    "\n",
    "data = [\n",
    "    [ emp   , emp, emp, emp, emp, emp, emp, emp, ], # 何も買ってない(初めての買い物客)\n",
    "    [ [   1], [0], [0], [0], [1], [0], [0], [0], ], # id:1 の商品を0°曜日に買った\n",
    "    [ [   1], [0], [0], [1], [0], [0], [0], [0], ], # id:1 の商品を90°曜日に買った\n",
    "    [ [9999], [0], [0], [0], [1], [0], [0], [0], ], # id:9999 の商品を0°曜日に買った\n",
    "\n",
    "    #[ 999999999, 1, [60], [0], [0], [0], [0], [0], [0], [0], [0], 1, ],\n",
    "    #[ 999999999, 1, [90], [0], [0], [0], [0], [0], [0], [0], [0], 1, ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c657ef-aea1-48b4-b07d-725392f8d788",
   "metadata": {},
   "source": [
    "推論で推薦アイテム一覧を計算する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12283ddc-8691-49ef-98f8-1547c9ab89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3    4    5    8    9   12   10    7    6   14   15   11   23   27\n",
      "    24   20   32   16   18   22   19   26   21   65   13   75   36  228\n",
      "    77   29  262   53   38   83  179   25   28   79   30   34   72   48\n",
      "    40  176   96   98   33   44  414   67   87  129   45   41  102  728\n",
      "    52   46  149  601  308   89  131  319   37 1069   57   56   82   43\n",
      "   215  109   84  249  362  261   73  142  115  135   17  108  349   51\n",
      "   150   54  457  144  171   39  104   80   97  447  240  967  117  605\n",
      "   590   91]\n",
      " [   4    3    5    8   10   12    9   14   20   15   27    7    6   32\n",
      "    11   36   13   23   24   37   18   16   53   72   83  215  142   29\n",
      "    19  228   21   75  117   22   26   28  126  308   39  179   98   48\n",
      "    25   97   58  601  728 1069   65   79  262   46   17   77   30  149\n",
      "    38   31   34  114  171   87  108  319  414  210  252   41  102  284\n",
      "    96   84  104  209   90 1479   44  998  249   40  472  261   67   91\n",
      "   345   33  281   45  349  197   50  475   52  782   95 1491  939  176\n",
      "   594  140]\n",
      " [   4    5   12    3   27   10   37    8  142  215   11  117  284   39\n",
      "   728   14  308  210  171   58  114    6    9  252  510 1069 1683  588\n",
      "    20  847  998    7 1479 1515  179  601  612  281  126   95  631  824\n",
      "    32 2592  900 1726  563   15   97  472 1261 1297  271  199  345 1039\n",
      "    75  323  414  594  710 1015   13  319  493   18  981  120 1052 1904\n",
      "   149   90  766 1081  354  121 1557   23  626  183 2186  361  124  811\n",
      "  1915  532 1008  261  349  209 1633  939  197 1988 2242 1484  389  987\n",
      "  1216 1361]\n",
      " [   4    3    5    8   10   12    9   14   15   20    7    6   27   32\n",
      "    23   24   11   36   13   21   29   16   19   18   83   75  262   25\n",
      "    53   22   26   72   28   48   77   65  179   79  228   98   37   41\n",
      "   728   46  102  249  319   96   40   38 1069   52   17  149  414   45\n",
      "    87   34   31   30  108   84  142  215  308  601  104   73   50  176\n",
      "    91   39  349  261   33  129   67 1491  117  138 1479   61   44  189\n",
      "    80  644   88   89  457  126  131  590  967   58   82   97  345  115\n",
      "    62  998]]\n"
     ]
    }
   ],
   "source": [
    "# 学習済みモデルを読み込む\n",
    "\n",
    "model = cloudpickle.load(open(os.path.join(MODEL_PATH, \"t4rec_model_class.pkl\"), \"rb\"))\n",
    "\n",
    "# setup the trainer\n",
    "\n",
    "# Define categorical and continuous columns to fed to training model\n",
    "x_cat_names = ['product_id-list', 'category_id-list', 'brand-list']\n",
    "x_cont_names = ['product_recency_days_log_norm-list', 'et_dayofweek_sin-list', 'et_dayofweek_cos-list',\n",
    "                'price_log_norm-list', 'relative_price_to_avg_categ_id-list']\n",
    "\n",
    "train = Dataset(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
    "schema = train.schema\n",
    "schema = schema.select_by_name(x_cat_names + x_cont_names)\n",
    "\n",
    "#Set arguments for training\n",
    "training_args = T4RecTrainingArguments(\n",
    "            output_dir = \"./tmp\",\n",
    "            max_sequence_length=20,\n",
    "            data_loader_engine='merlin',\n",
    "            num_train_epochs=3,\n",
    "            dataloader_drop_last=False,\n",
    "            per_device_train_batch_size = 256,\n",
    "            per_device_eval_batch_size = 32,\n",
    "            gradient_accumulation_steps = 1,\n",
    "            learning_rate=0.000666,\n",
    "            report_to = [],\n",
    "            logging_steps=200,\n",
    "        )\n",
    "\n",
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")\n",
    "\n",
    "\n",
    "# predict\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ds = Dataset(df)\n",
    "    out = trainer.predict(ds)\n",
    "    items = out.predictions[0]\n",
    "    print(items)\n",
    "    #logits = out.predictions[1]\n",
    "    #for i in range(len(items)):\n",
    "    #    print(f\"  item#{i}   {items[i]}\")\n",
    "    #    print(f\"  logits#{i} {logits[i]}\")\n",
    "    #    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9356b8-5f41-43f2-afd5-4f196a53dead",
   "metadata": {},
   "source": [
    "出力は、各セッションに対する推薦アイテム100個。\n",
    "Workflowで変換されたアイテムIDで、先頭のものほど推薦度合が強い。\n",
    "また若い番号ほど頻出するので推薦されやすく、かつ推薦度合が強くなりやすいことに留意が必要。\n",
    "\n",
    "細かい条件で推薦順位が変わっていることがわかる。\n",
    "特に3番目は2番目と同じアイテムに興味があるユーザーに対して、\n",
    "異なる曜日には異なるアイテム(`3` ではなく `37`)を推薦していることが見て取れる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
