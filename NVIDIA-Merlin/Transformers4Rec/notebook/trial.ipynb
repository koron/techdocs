{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76ab823-5cad-495e-8fd4-fa8f939a507f",
   "metadata": {},
   "source": [
    "# 本ディレクトリの一連の流れを試せるノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573ae0a-4837-43d0-9f46-55b5bdb781f8",
   "metadata": {},
   "source": [
    "## 諸々の準備\n",
    "\n",
    "まずはインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0553674-7432-4341-a2c3-81fe0c0a791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle\n",
    "import torch\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Operator\n",
    "\n",
    "from merlin.dag import ColumnSelector\n",
    "from merlin.schema import Schema, Tags\n",
    "from merlin.schema import Schema\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# numba からの警告を抑制する\n",
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0\n",
    "\n",
    "import transformers4rec.torch as tr\n",
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch import Trainer\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt\n",
    "from transformers4rec.torch.utils.examples_utils import wipe_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224e07e-f29f-4898-b228-10bdb1174c12",
   "metadata": {},
   "source": [
    "各種定数を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ca665a-3147-4087-ab40-b54397261afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data/\")\n",
    "CSV_PATH = os.path.join(INPUT_DATA_DIR, \"2019-Oct.csv\")\n",
    "PARQUET_PATH = os.path.join(INPUT_DATA_DIR, \"Oct-2019.parquet\")\n",
    "PROCESSED_PATH = os.path.join(INPUT_DATA_DIR, \"processed_nvt\")\n",
    "WORKFLOW_PATH = os.path.join(INPUT_DATA_DIR, 'workflow_etl')\n",
    "SESSIONS_PATH = os.path.join(INPUT_DATA_DIR, \"sessions_by_day\")\n",
    "MODEL_PATH = os.path.join(INPUT_DATA_DIR, \"trained_model\")\n",
    "\n",
    "SESSIONS_MAX_LENGTH = 20\n",
    "MINIMUM_SESSION_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67394f2-7c45-4b8e-92aa-a8d67fba05cb",
   "metadata": {},
   "source": [
    "## 学習データの準備\n",
    "\n",
    "実験のためのデータをダウンロードする。\n",
    "使うデータは <https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store> の `2019-Oct.csv` で、\n",
    "サイズは5.3GiBになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9149dce-a9da-46e3-aa02-eb3f040426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CSV_PATH):\n",
    "    # TODO: データをCSV_PATHへダウンロードする\n",
    "    print(\"Download 2019-Oct.csv from https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944950f-04ce-4379-9a9b-efabf97739fd",
   "metadata": {},
   "source": [
    "CSVのままでは扱いにくいので Parquet 形式に変換する。\n",
    "初めて変換する場合は特にDocker環境では5分ほどかかる。\n",
    "変換結果はファイルに保存するので、そのファイル `Oct-2019.parquet` がある場合は自動的にこのステップをスキップする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3757a1-f68e-4242-8f00-62bd694555cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count with in-session repeated interactions: 42448762\n",
      "Count after removed in-session repeated interactions: 30733301\n",
      "CPU times: user 1min 6s, sys: 17 s, total: 1min 23s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(PARQUET_PATH):\n",
    "    # CSVをロードする。Docker上でやると3～5分くらいかかる\n",
    "    raw_df = cudf.read_csv(CSV_PATH)\n",
    "    \n",
    "    # タイムスタンプの形式を秒へ変換\n",
    "    raw_df['event_time_dt'] = raw_df['event_time'].astype('datetime64[s]')\n",
    "    raw_df['event_time_ts'] = raw_df['event_time_dt'].astype('int')\n",
    "    \n",
    "    # `user_session` カラムが null の行を削除\n",
    "    raw_df = raw_df[raw_df['user_session'].isnull()==False]\n",
    "    \n",
    "    # `event_time` カラムは利用しない\n",
    "    raw_df = raw_df.drop(['event_time'], axis=1)\n",
    "    \n",
    "    # Workflowを用いて `user_session` カラムでグルーピングしデータフレームへ変換する\n",
    "    cols = list(raw_df.columns)\n",
    "    cols.remove('user_session')\n",
    "    df_event = nvt.Dataset(raw_df) \n",
    "    cat_feats = ['user_session'] >> nvt.ops.Categorify()\n",
    "    workflow = nvt.Workflow(cols + cat_feats)\n",
    "    workflow.fit(df_event)\n",
    "    df = workflow.transform(df_event).to_ddf().compute()\n",
    "    \n",
    "    # データ読み込みに利用していたメモリを解放する\n",
    "    raw_df = None\n",
    "    del(raw_df)\n",
    "    gc.collect()\n",
    "    \n",
    "    # 連続した (user, item) のインタラクションを削除\n",
    "    df = df.sort_values(['user_session', 'event_time_ts']).reset_index(drop=True)\n",
    "    print(\"Count with in-session repeated interactions: {}\".format(len(df)))\n",
    "    # Sorts the dataframe by session and timestamp, to remove consecutive repetitions\n",
    "    df['product_id_past'] = df['product_id'].shift(1).fillna(0)\n",
    "    df['session_id_past'] = df['user_session'].shift(1).fillna(0)\n",
    "    #Keeping only no consecutive repeated in session interactions\n",
    "    df = df[~((df['user_session'] == df['session_id_past']) & \\\n",
    "                 (df['product_id'] == df['product_id_past']))]\n",
    "    print(\"Count after removed in-session repeated interactions: {}\".format(len(df)))\n",
    "    del(df['product_id_past'])\n",
    "    del(df['session_id_past'])\n",
    "    gc.collect()\n",
    "    \n",
    "    # 特定の item が最初に表れた時刻を記録するカラムを追加\n",
    "    item_first_interaction_df = df.groupby('product_id').agg({'event_time_ts': 'min'}) \\\n",
    "                .reset_index().rename(columns={'event_time_ts': 'prod_first_event_time_ts'})\n",
    "    gc.collect()\n",
    "    df = df.merge(item_first_interaction_df, on=['product_id'], how='left').reset_index(drop=True)\n",
    "    item_first_interaction_df=None\n",
    "    del(item_first_interaction_df)\n",
    "    gc.collect()\n",
    "\n",
    "    # 最初の1週間分のデータだけを使う\n",
    "    df = df[df['event_time_dt'] < np.datetime64('2019-10-08')].reset_index(drop=True)\n",
    "    # それが済めば `event_time_dt` カラムは不要なので削除する\n",
    "    df = df.drop(['event_time_dt'], axis=1)\n",
    "\n",
    "    # Parquet形式にして書き出す\n",
    "    df.to_parquet(PARQUET_PATH)\n",
    "\n",
    "    df = None\n",
    "    del(df)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0f903-35f3-4d0b-bfe1-f1f870f5647f",
   "metadata": {},
   "source": [
    "## Workflowを使って ETL する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13631ae7-fa02-4612-b962-c0c4b7d5514a",
   "metadata": {},
   "source": [
    "以下はWorkflowを構築する手続き。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80bd35c-49dd-4d3d-839e-bf05325fafad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91.2 ms, sys: 48.9 ms, total: 140 ms\n",
      "Wall time: 614 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# カテゴリ化するカラムを指定する\n",
    "item_id = ['product_id'] >> nvt.ops.TagAsItemID()\n",
    "cat_feats = item_id + ['category_code', 'brand', 'user_id', 'category_id', 'event_type'] >> nvt.ops.Categorify()\n",
    "\n",
    "\n",
    "# 時刻に関するカラムを変換する\n",
    "\n",
    "session_ts = ['event_time_ts']\n",
    "\n",
    "session_time = (\n",
    "    session_ts >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "\n",
    "sessiontime_weekday = (\n",
    "    session_time >> \n",
    "    nvt.ops.LambdaOp(lambda col: col.dt.weekday) >> \n",
    "    nvt.ops.Rename(name ='et_dayofweek')\n",
    ")\n",
    "\n",
    "\n",
    "def get_cycled_feature_value_sin(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_sin = np.sin(2*np.pi*value_scaled)\n",
    "    return value_sin\n",
    "\n",
    "def get_cycled_feature_value_cos(col, max_value):\n",
    "    value_scaled = (col + 0.000001) / max_value\n",
    "    value_cos = np.cos(2*np.pi*value_scaled)\n",
    "    return value_cos\n",
    "\n",
    "weekday_sin = (sessiontime_weekday >> \n",
    "               (lambda col: get_cycled_feature_value_sin(col+1, 7)) >> \n",
    "               nvt.ops.Rename(name = 'et_dayofweek_sin') >>\n",
    "               nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    "              )\n",
    "    \n",
    "weekday_cos= (sessiontime_weekday >> \n",
    "              (lambda col: get_cycled_feature_value_cos(col+1, 7)) >> \n",
    "              nvt.ops.Rename(name = 'et_dayofweek_cos') >>\n",
    "              nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    "             )\n",
    "\n",
    "# アイテムの最新性を計算するためのカスタムオペレーター\n",
    "class ItemRecency(nvt.ops.Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "        for column in columns.names:\n",
    "            col = gdf[column]\n",
    "            item_first_timestamp = gdf['prod_first_event_time_ts']\n",
    "            delta_days = (col - item_first_timestamp) / (60*60*24)\n",
    "            gdf[column + \"_age_days\"] = delta_days * (delta_days >=0)\n",
    "        return gdf\n",
    "\n",
    "    def compute_selector(\n",
    "        self,\n",
    "        input_schema: Schema,\n",
    "        selector: ColumnSelector,\n",
    "        parents_selector: ColumnSelector,\n",
    "        dependencies_selector: ColumnSelector,\n",
    "    ) -> ColumnSelector:\n",
    "        self._validate_matching_cols(input_schema, parents_selector, \"computing input selector\")\n",
    "        return parents_selector\n",
    "\n",
    "    def column_mapping(self, col_selector):\n",
    "        column_mapping = {}\n",
    "        for col_name in col_selector.names:\n",
    "            column_mapping[col_name + \"_age_days\"] = [col_name]\n",
    "        return column_mapping\n",
    "\n",
    "    @property\n",
    "    def dependencies(self):\n",
    "        return [\"prod_first_event_time_ts\"]\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        return np.float64\n",
    "\n",
    "recency_features = ['event_time_ts'] >> ItemRecency() \n",
    "recency_features_norm = (recency_features >> \n",
    "                         nvt.ops.LogOp() >> \n",
    "                         nvt.ops.Normalize(out_dtype=np.float32) >> \n",
    "                         nvt.ops.Rename(name='product_recency_days_log_norm')\n",
    "                        )\n",
    "\n",
    "time_features = (\n",
    "    session_time +\n",
    "    sessiontime_weekday +\n",
    "    weekday_sin +\n",
    "    weekday_cos +\n",
    "    recency_features_norm\n",
    ")\n",
    "\n",
    "# ロング・テイルな価格のカラムを標準化する\n",
    "price_log = ['price'] >> nvt.ops.LogOp() >> nvt.ops.Normalize(out_dtype=np.float32) >> nvt.ops.Rename(name='price_log_norm')\n",
    "\n",
    "# 相対価格をカテゴリー(ID)の平均に変換する\n",
    "\n",
    "def relative_price_to_avg_categ(col, gdf):\n",
    "    epsilon = 1e-5\n",
    "    col = ((gdf['price'] - col) / (col + epsilon)) * (col > 0).astype(int)\n",
    "    return col\n",
    "    \n",
    "avg_category_id_pr = ['category_id'] >> nvt.ops.JoinGroupby(cont_cols =['price'], stats=[\"mean\"]) >> nvt.ops.Rename(name='avg_category_id_price')\n",
    "relative_price_to_avg_category = (\n",
    "    avg_category_id_pr >> \n",
    "    nvt.ops.LambdaOp(relative_price_to_avg_categ, dependency=['price']) >> \n",
    "    nvt.ops.Rename(name=\"relative_price_to_avg_categ_id\") >>\n",
    "    nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    ")\n",
    "\n",
    "# インタラクションをセッションでグルーピングする\n",
    "groupby_feats = ['event_time_ts', 'user_session'] + cat_feats + time_features + price_log + relative_price_to_avg_category\n",
    "\n",
    "groupby_features = groupby_feats >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"user_session\"], \n",
    "    sort_cols=[\"event_time_ts\"],\n",
    "    aggs={\n",
    "        'user_id': ['first'],\n",
    "        'product_id': [\"list\", \"count\"],\n",
    "        'category_code': [\"list\"],  \n",
    "        'brand': [\"list\"], \n",
    "        'category_id': [\"list\"], \n",
    "        'event_time_ts': [\"first\"],\n",
    "        'event_time_dt': [\"first\"],\n",
    "        'et_dayofweek_sin': [\"list\"],\n",
    "        'et_dayofweek_cos': [\"list\"],\n",
    "        'price_log_norm': [\"list\"],\n",
    "        'relative_price_to_avg_categ_id': [\"list\"],\n",
    "        'product_recency_days_log_norm': [\"list\"]\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "\n",
    "groupby_features_list = groupby_features['product_id-list',\n",
    "        'category_code-list',  \n",
    "        'brand-list', \n",
    "        'category_id-list', \n",
    "        'et_dayofweek_sin-list',\n",
    "        'et_dayofweek_cos-list',\n",
    "        'price_log_norm-list',\n",
    "        'relative_price_to_avg_categ_id-list',\n",
    "        'product_recency_days_log_norm-list']\n",
    "\n",
    "groupby_features_trim = groupby_features_list >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH, pad=True)\n",
    "\n",
    "# calculate session day index based on 'timestamp-first' column\n",
    "day_index = ((groupby_features['event_time_dt-first'])  >> \n",
    "             nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days +1) >> \n",
    "             nvt.ops.Rename(f = lambda col: \"day_index\") >>\n",
    "             nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    "            )\n",
    "\n",
    "sess_id = groupby_features['user_session'] >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    "\n",
    "selected_features = sess_id + groupby_features['product_id-count'] + groupby_features_trim + day_index\n",
    "\n",
    "# 変換済みのデータを再度、読み込む\n",
    "df = cudf.read_parquet(PARQUET_PATH)\n",
    "\n",
    "filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df[\"product_id-count\"] >= MINIMUM_SESSION_LENGTH)\n",
    "\n",
    "workflow = nvt.Workflow(filtered_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f372c7-7a15-45b5-9236-27fb4c477594",
   "metadata": {},
   "source": [
    "作ったWorkflowで ETL (Extract, Transform, Load) する。\n",
    "結果は `processed_nvt` ディレクトリに保存する。\n",
    "既に `processed_nvt` ディレクトリが存在する場合は自動的にこのステップをスキップする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab5050a-efd6-4a9c-8f15-dcdd10e81018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.47 s, sys: 515 ms, total: 7.99 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(PROCESSED_PATH):\n",
    "    dataset = nvt.Dataset(df)\n",
    "    # Learn features statistics necessary of the preprocessing workflow\n",
    "    # The following will generate schema.pbtxt file in the provided folder and export the parquet files.\n",
    "    workflow.fit_transform(dataset).to_parquet(PROCESSED_PATH)\n",
    "    dataset = None\n",
    "    del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893d1ecd-84a3-4b0a-a02d-68eb956ff38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作った Workflow はディレクトリへ保存しておく\n",
    "if not os.path.exists(WORKFLOW_PATH):\n",
    "    workflow.save(WORKFLOW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4dcc2-0ae4-4154-87e9-476c7e0117a6",
   "metadata": {},
   "source": [
    "## データを日ごとに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d757a3e-533b-4c77-a1ed-8d1702a27353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|█████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 554 ms, total: 4.14 s\n",
      "Wall time: 6.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SESSIONS_PATH = os.path.join(INPUT_DATA_DIR, \"sessions_by_day\")\n",
    "\n",
    "if not os.path.exists(SESSIONS_PATH):\n",
    "    PARTITION_COL = 'day_index'\n",
    "    \n",
    "    OUTPUT_FOLDER = os.environ.get(\"OUTPUT_FOLDER\", SESSIONS_PATH)\n",
    "    !mkdir -p $OUTPUT_FOLDER\n",
    "    \n",
    "    # read in the processed train dataset\n",
    "    sessions_gdf = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
    "    \n",
    "    from transformers4rec.utils.data_utils import save_time_based_splits\n",
    "    save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
    "                           output_dir= OUTPUT_FOLDER,\n",
    "                           partition_col=PARTITION_COL,\n",
    "                           timestamp_col='user_session', \n",
    "                          )\n",
    "\n",
    "    sessions_gdf = None\n",
    "    del(sessions_gdf)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc714197-2fb5-4ba2-bdf5-5af8ccaf20f0",
   "metadata": {},
   "source": [
    "## Transformerモデルを作り学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e561809-17b8-4d31-a073-d66e3b2121db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting inputs of NextItemPredictionTask to'64' As weight tying requires the input dimension '192' to be equal to the item-id embedding dimension '64'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1314/1314 00:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>9.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>8.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>8.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>8.420100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1285' max='415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [415/415 01:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 2 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 8.580676078796387\n",
      " eval_/next-item/ndcg_at_10 = 0.05574140325188637\n",
      " eval_/next-item/ndcg_at_20 = 0.06698620319366455\n",
      " eval_/next-item/recall_at_10 = 0.10406454652547836\n",
      " eval_/next-item/recall_at_20 = 0.14863131940364838\n",
      " eval_runtime = 6.7568\n",
      " eval_samples_per_second = 1965.438\n",
      " eval_steps_per_second = 61.42\n",
      "********************\n",
      "Launch training for day 2 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1245' max='1245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1245/1245 00:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>8.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>8.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>7.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>7.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>7.471100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 3 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 7.884793281555176\n",
      " eval_/next-item/ndcg_at_10 = 0.08094239234924316\n",
      " eval_/next-item/ndcg_at_20 = 0.09908241778612137\n",
      " eval_/next-item/recall_at_10 = 0.15609200298786163\n",
      " eval_/next-item/recall_at_20 = 0.2280215322971344\n",
      " eval_runtime = 6.8796\n",
      " eval_samples_per_second = 1786.159\n",
      " eval_steps_per_second = 55.817\n",
      "********************\n",
      "Launch training for day 3 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>7.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>7.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.093300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 4 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 7.466495513916016\n",
      " eval_/next-item/ndcg_at_10 = 0.09928520768880844\n",
      " eval_/next-item/ndcg_at_20 = 0.12029159814119339\n",
      " eval_/next-item/recall_at_10 = 0.1897706836462021\n",
      " eval_/next-item/recall_at_20 = 0.272932231426239\n",
      " eval_runtime = 8.4544\n",
      " eval_samples_per_second = 1839.52\n",
      " eval_steps_per_second = 57.485\n",
      "CPU times: user 3min 25s, sys: 28.7 s, total: 3min 54s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):    \n",
    "    # 入力スキーマの構築\n",
    "    \n",
    "    # Define categorical and continuous columns to fed to training model\n",
    "    x_cat_names = ['product_id-list', 'category_id-list', 'brand-list']\n",
    "    x_cont_names = ['product_recency_days_log_norm-list', 'et_dayofweek_sin-list', 'et_dayofweek_cos-list', \n",
    "                    'price_log_norm-list', 'relative_price_to_avg_categ_id-list']\n",
    "    \n",
    "    train = Dataset(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
    "    schema = train.schema\n",
    "    schema = schema.select_by_name(x_cat_names + x_cont_names)\n",
    "    \n",
    "    # モデル情報の構築\n",
    "    \n",
    "    # Define input block\n",
    "    sequence_length, d_model = 20, 192\n",
    "    # Define input module to process tabular input-features and to prepare masked inputs\n",
    "    inputs= tr.TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=sequence_length,\n",
    "        aggregation=\"concat\",\n",
    "        d_output=d_model,\n",
    "        masking=\"mlm\",\n",
    "    )\n",
    "    \n",
    "    # Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "    transformer_config = tr.XLNetConfig.build(\n",
    "        d_model=d_model, n_head=4, n_layer=2, total_seq_length=sequence_length\n",
    "    )\n",
    "    # Define the model block including: inputs, masking, projection and transformer block.\n",
    "    body = tr.SequentialBlock(\n",
    "        inputs, tr.MLPBlock([192]), tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    "    )\n",
    "    \n",
    "    # Define the head related to next item prediction task \n",
    "    head = tr.Head(\n",
    "        body,\n",
    "        tr.NextItemPredictionTask(weight_tying=True, \n",
    "                                         metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),  \n",
    "                                                  RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n",
    "    )\n",
    "    \n",
    "    # Get the end-to-end Model class \n",
    "    model = tr.Model(head)\n",
    "    \n",
    "    #Set arguments for training\n",
    "    training_args = T4RecTrainingArguments(\n",
    "                output_dir = \"./tmp\",\n",
    "                max_sequence_length=20,\n",
    "                data_loader_engine='merlin',\n",
    "                num_train_epochs=3,\n",
    "                dataloader_drop_last=False,\n",
    "                per_device_train_batch_size = 256,\n",
    "                per_device_eval_batch_size = 32,\n",
    "                gradient_accumulation_steps = 1,\n",
    "                learning_rate=0.000666,\n",
    "                report_to = [],\n",
    "                logging_steps=200,\n",
    "            )\n",
    "    \n",
    "    # 学習を実行し、結果を保存する\n",
    "    \n",
    "    # Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        schema=schema,\n",
    "        compute_metrics=True,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", SESSIONS_PATH)\n",
    "    \n",
    "    start_time_window_index = 1\n",
    "    final_time_window_index = 4\n",
    "    for time_index in range(start_time_window_index, final_time_window_index):\n",
    "        # Set data \n",
    "        time_index_train = time_index\n",
    "        time_index_eval = time_index + 1\n",
    "        train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n",
    "        eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
    "        # Train on day related to time_index \n",
    "        print('*'*20)\n",
    "        print(\"Launch training for day %s are:\" %time_index)\n",
    "        print('*'*20 + '\\n')\n",
    "        trainer.train_dataset_or_path = train_paths\n",
    "        trainer.reset_lr_scheduler()\n",
    "        trainer.train()\n",
    "        trainer.state.global_step +=1\n",
    "        # Evaluate on the following day\n",
    "        trainer.eval_dataset_or_path = eval_paths\n",
    "        train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "        print('*'*20)\n",
    "        print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "        print('\\n' + '*'*20 + '\\n')\n",
    "        for key in sorted(train_metrics.keys()):\n",
    "            print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "        wipe_memory()\n",
    "    \n",
    "    model.save(MODEL_PATH)\n",
    "\n",
    "    model = None\n",
    "    del(model)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5047a-a6ec-4aee-8785-1abbd09cc5ab",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n",
    "人工的に作ったセッションデータから、推薦アイテム列を推論(作成)する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce69f39-7e41-4900-81e0-6c37cc957fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose dataframe\n",
    "\n",
    "cols = [\n",
    "        'product_id-list',\n",
    "        'brand-list',\n",
    "        'category_id-list',\n",
    "        'et_dayofweek_sin-list',\n",
    "        'et_dayofweek_cos-list',\n",
    "        'price_log_norm-list',\n",
    "        'relative_price_to_avg_categ_id-list',\n",
    "        'product_recency_days_log_norm-list',\n",
    "        ]\n",
    "\n",
    "emp = np.empty(0, dtype=np.int64)\n",
    "\n",
    "data = [\n",
    "    [ emp   , emp, emp, emp, emp, emp, emp, emp, ], # 何も買ってない(初めての買い物客)\n",
    "    [ [   1], [0], [0], [0], [1], [0], [0], [0], ], # id:1 の商品を0°曜日に買った\n",
    "    [ [   1], [0], [0], [1], [0], [0], [0], [0], ], # id:1 の商品を90°曜日に買った\n",
    "    [ [9999], [0], [0], [0], [1], [0], [0], [0], ], # id:9999 の商品を0°曜日に買った\n",
    "\n",
    "    #[ 999999999, 1, [60], [0], [0], [0], [0], [0], [0], [0], [0], 1, ],\n",
    "    #[ 999999999, 1, [90], [0], [0], [0], [0], [0], [0], [0], [0], 1, ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c657ef-aea1-48b4-b07d-725392f8d788",
   "metadata": {},
   "source": [
    "推論で推薦アイテム一覧を計算する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12283ddc-8691-49ef-98f8-1547c9ab89a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3    7    6    9    5    8    4   12   11   16   18   19   23   22\n",
      "    27   14   30   34   38   10   13   26   37   15   17   33   20  644\n",
      "    43   75   39   44   67   32  331   65  349  179   24   56   21   64\n",
      "   120  124  109  414   54  834  261  149  249  601   57  308   82  121\n",
      "    29   84   66   60   87   59  499  131   53  106   95   96  967  204\n",
      "    58  115 1942  160   89  262  401  193  103  148  135  509  403  280\n",
      "  1219   72  202  512  144   69  263  248  104  997  867 1734  189   40\n",
      "    51 1477]\n",
      " [   3   12    4    5    8   27    6    7    9   11  644   14   10   37\n",
      "   349   75   17   13  249   30  149   20  179   39  559  189  104   15\n",
      "    32   16   19  331  261  414   23  806  120  601  284  124   44  138\n",
      "    65  403  834  817  453   96 1942  499 1477  334   49  262   58   95\n",
      "    18 1219  997  967  394  280  441  115  680  308  510  358 1039  981\n",
      "  2186 1529  401  457  509  121  597   22  512 1028  867   71   34  252\n",
      "   728   21   48  202  508  193  242  990 1683 1555   38   59  472  841\n",
      "  1643   29]\n",
      " [  12   27   37  510  284 1515  900   70 1015    4 1683 1904 1039 1297\n",
      "     5 1216 1726  120  806   39   95 2437 1206    3  349  124   58 1529\n",
      "   644  472  252  215  559  280  149 1634  487    8  601  114 1972  981\n",
      "  1477 1915  171  308  453  530   49  987  824  395  142 1008 1581  325\n",
      "   234  104  990   75  179  509   11 2282  574  811 2190 1261 1792  865\n",
      "  1749  187  728 1723   71 3371  193 1727 1942  354 1045  703  389  847\n",
      "   729 3344 1069  695  189  249 1768  261   10  253  121 1461  237  997\n",
      "   485  680]\n",
      " [   3    5    8    4   12   27    6    7    9   11   14   10  644   75\n",
      "    30   16   20  349   15   23   37   13   19   17  149  179   32  249\n",
      "    18  414   39  261   44   22  834  104  189  331  262   34  499  403\n",
      "    65  124  120   38  601  817  457   21   24  559   96   58  284  997\n",
      "    95  138 1219  806  680  115  597   26  967   29 1477  867 1028   48\n",
      "    53  508   84  308  280   33  512 1942  121  453  202  334 1258 1339\n",
      "   401   43  509  981   72   36  394  441  252   59  193  389 2186   28\n",
      "   131 1342]]\n"
     ]
    }
   ],
   "source": [
    "# 学習済みモデルを読み込む\n",
    "\n",
    "model = cloudpickle.load(open(os.path.join(MODEL_PATH, \"t4rec_model_class.pkl\"), \"rb\"))\n",
    "\n",
    "# setup the trainer\n",
    "\n",
    "# Define categorical and continuous columns to fed to training model\n",
    "x_cat_names = ['product_id-list', 'category_id-list', 'brand-list']\n",
    "x_cont_names = ['product_recency_days_log_norm-list', 'et_dayofweek_sin-list', 'et_dayofweek_cos-list',\n",
    "                'price_log_norm-list', 'relative_price_to_avg_categ_id-list']\n",
    "\n",
    "train = Dataset(os.path.join(INPUT_DATA_DIR, \"processed_nvt/part_0.parquet\"))\n",
    "schema = train.schema\n",
    "schema = schema.select_by_name(x_cat_names + x_cont_names)\n",
    "\n",
    "#Set arguments for training\n",
    "training_args = T4RecTrainingArguments(\n",
    "            output_dir = \"./tmp\",\n",
    "            max_sequence_length=20,\n",
    "            data_loader_engine='merlin',\n",
    "            num_train_epochs=3,\n",
    "            dataloader_drop_last=False,\n",
    "            per_device_train_batch_size = 256,\n",
    "            per_device_eval_batch_size = 32,\n",
    "            gradient_accumulation_steps = 1,\n",
    "            learning_rate=0.000666,\n",
    "            report_to = [],\n",
    "            logging_steps=200,\n",
    "        )\n",
    "\n",
    "# Instantiate the T4Rec Trainer, which manages training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")\n",
    "\n",
    "\n",
    "# predict\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ds = Dataset(df)\n",
    "    out = trainer.predict(ds)\n",
    "    items = out.predictions[0]\n",
    "    print(items)\n",
    "    #logits = out.predictions[1]\n",
    "    #for i in range(len(items)):\n",
    "    #    print(f\"  item#{i}   {items[i]}\")\n",
    "    #    print(f\"  logits#{i} {logits[i]}\")\n",
    "    #    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9356b8-5f41-43f2-afd5-4f196a53dead",
   "metadata": {},
   "source": [
    "出力は、各セッションに対する推薦アイテム100個。\n",
    "Workflowで変換されたアイテムIDで、先頭のものほど推薦度合が強い。\n",
    "また若い番号ほど頻出するので推薦されやすく、かつ推薦度合が強くなりやすいことに留意が必要。\n",
    "\n",
    "細かい条件で推薦順位が変わっていることがわかる。\n",
    "特に3番目は2番目と同じアイテムに興味があるユーザーに対して、\n",
    "異なる曜日には異なるアイテム(`3` ではなく `37`)を推薦していることが見て取れる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
